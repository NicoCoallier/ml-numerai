{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d605b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_addons.activations import sparsemax\n",
    "from typing import *\n",
    "\n",
    "import os \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02cc40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/google-research/google-research/blob/master/tabnet/tabnet_model.py\n",
    "def glu(x, n_units=None):\n",
    "    \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n",
    "    return x[:, :n_units] * tf.nn.sigmoid(x[:, n_units:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a51f1f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.05249792, 0.10996681, 0.23947506]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glu(tf.constant([[0.1,0.2,0.4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b9ec5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GhostBatchNormalization(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, virtual_divider: int = 1, momentum: float = 0.9, epsilon: float = 1e-5\n",
    "    ):\n",
    "        super(GhostBatchNormalization, self).__init__()\n",
    "        self.virtual_divider = virtual_divider\n",
    "        self.bn = BatchNormInferenceWeighting(momentum=momentum)\n",
    "\n",
    "    def call(self, x, training: bool = None, alpha: float = 0.0):\n",
    "        if training:\n",
    "            chunks = tf.split(x, self.virtual_divider)\n",
    "            x = [self.bn(x, training=True) for x in chunks]\n",
    "            return tf.concat(x, 0)\n",
    "        return self.bn(x, training=False, alpha=alpha)\n",
    "\n",
    "    @property\n",
    "    def moving_mean(self):\n",
    "        return self.bn.moving_mean\n",
    "\n",
    "    @property\n",
    "    def moving_variance(self):\n",
    "        return self.bn.moving_variance\n",
    "\n",
    "\n",
    "class BatchNormInferenceWeighting(tf.keras.layers.Layer):\n",
    "    def __init__(self, momentum: float = 0.9, epsilon: float = None):\n",
    "        super(BatchNormInferenceWeighting, self).__init__()\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = tf.keras.backend.epsilon() if epsilon is None else epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "\n",
    "        self.gamma = tf.Variable(\n",
    "            initial_value=tf.ones((channels,), tf.float32), trainable=True,\n",
    "        )\n",
    "        self.beta = tf.Variable(\n",
    "            initial_value=tf.zeros((channels,), tf.float32), trainable=True,\n",
    "        )\n",
    "\n",
    "        self.moving_mean = tf.Variable(\n",
    "            initial_value=tf.zeros((channels,), tf.float32), trainable=False,\n",
    "        )\n",
    "        self.moving_mean_of_squares = tf.Variable(\n",
    "            initial_value=tf.zeros((channels,), tf.float32), trainable=False,\n",
    "        )\n",
    "\n",
    "    def __update_moving(self, var, value):\n",
    "        var.assign(var * self.momentum + (1 - self.momentum) * value)\n",
    "\n",
    "    def __apply_normalization(self, x, mean, variance):\n",
    "        return self.gamma * (x - mean) / tf.sqrt(variance + self.epsilon) + self.beta\n",
    "\n",
    "    def call(self, x, training: bool = None, alpha: float = 0.0):\n",
    "        mean = tf.reduce_mean(x, axis=0)\n",
    "        mean_of_squares = tf.reduce_mean(tf.pow(x, 2), axis=0)\n",
    "\n",
    "        if training:\n",
    "            # update moving stats\n",
    "            self.__update_moving(self.moving_mean, mean)\n",
    "            self.__update_moving(self.moving_mean_of_squares, mean_of_squares)\n",
    "\n",
    "            variance = mean_of_squares - tf.pow(mean, 2)\n",
    "            x = self.__apply_normalization(x, mean, variance)\n",
    "        else:\n",
    "            mean = alpha * mean + (1 - alpha) * self.moving_mean\n",
    "            variance = (\n",
    "                alpha * mean_of_squares + (1 - alpha) * self.moving_mean_of_squares\n",
    "            ) - tf.pow(mean, 2)\n",
    "            x = self.__apply_normalization(x, mean, variance)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12a690a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureBlock(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        apply_glu: bool = True,\n",
    "        bn_momentum: float = 0.9,\n",
    "        bn_virtual_divider: int = 32,\n",
    "        fc: tf.keras.layers.Layer = None,\n",
    "        epsilon: float = 1e-5,\n",
    "    ):\n",
    "        super(FeatureBlock, self).__init__()\n",
    "        self.apply_gpu = apply_glu\n",
    "        self.feature_dim = feature_dim\n",
    "        units = feature_dim * 2 if apply_glu else feature_dim\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(units, use_bias=False) if fc is None else fc\n",
    "        self.bn = GhostBatchNormalization(\n",
    "            virtual_divider=bn_virtual_divider, momentum=bn_momentum\n",
    "        )\n",
    "\n",
    "    def call(self, x, training: bool = None, alpha: float = 0.0):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x, training=training, alpha=alpha)\n",
    "        if self.apply_gpu:\n",
    "            return glu(x, self.feature_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentiveTransformer(tf.keras.Model):\n",
    "    def __init__(self, feature_dim: int, bn_momentum: float, bn_virtual_divider: int):\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.block = FeatureBlock(\n",
    "            feature_dim,\n",
    "            bn_momentum=bn_momentum,\n",
    "            bn_virtual_divider=bn_virtual_divider,\n",
    "            apply_glu=False,\n",
    "        )\n",
    "\n",
    "    def call(self, x, prior_scales, training=None, alpha: float = 0.0):\n",
    "        x = self.block(x, training=training, alpha=alpha)\n",
    "        return sparsemax(x * prior_scales)\n",
    "\n",
    "\n",
    "class FeatureTransformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        fcs: List[tf.keras.layers.Layer] = [],\n",
    "        n_total: int = 4,\n",
    "        n_shared: int = 2,\n",
    "        bn_momentum: float = 0.9,\n",
    "        bn_virtual_divider: int = 1,\n",
    "    ):\n",
    "        super(FeatureTransformer, self).__init__()\n",
    "        self.n_total, self.n_shared = n_total, n_shared\n",
    "\n",
    "        kargs = {\n",
    "            \"feature_dim\": feature_dim,\n",
    "            \"bn_momentum\": bn_momentum,\n",
    "            \"bn_virtual_divider\": bn_virtual_divider,\n",
    "        }\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks: List[FeatureBlock] = []\n",
    "        for n in range(n_total):\n",
    "            # some shared blocks\n",
    "            if fcs and n < len(fcs):\n",
    "                self.blocks.append(FeatureBlock(**kargs, fc=fcs[n]))\n",
    "            # build new blocks\n",
    "            else:\n",
    "                self.blocks.append(FeatureBlock(**kargs))\n",
    "\n",
    "    def call(\n",
    "        self, x: tf.Tensor, training: bool = None, alpha: float = 0.0\n",
    "    ) -> tf.Tensor:\n",
    "        x = self.blocks[0](x, training=training, alpha=alpha)\n",
    "        for n in range(1, self.n_total):\n",
    "            x = x * tf.sqrt(0.5) + self.blocks[n](x, training=training, alpha=alpha)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def shared_fcs(self):\n",
    "        return [self.blocks[i].fc for i in range(self.n_shared)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39df5970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNet(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        feature_dim: int,\n",
    "        output_dim: int,\n",
    "        feature_columns: List = None,\n",
    "        n_step: int = 1,\n",
    "        n_total: int = 4,\n",
    "        n_shared: int = 2,\n",
    "        relaxation_factor: float = 1.5,\n",
    "        bn_epsilon: float = 1e-5,\n",
    "        bn_momentum: float = 0.7,\n",
    "        bn_virtual_divider: int = 1,\n",
    "    ):\n",
    "        \"\"\"TabNet\n",
    "        Will output a vector of size output_dim.\n",
    "        Args:\n",
    "            num_features (int): Number of features.\n",
    "            feature_dim (int): Embedding feature dimention to use.\n",
    "            output_dim (int): Output dimension.\n",
    "            feature_columns (List, optional): If defined will add a DenseFeatures layer first. Defaults to None.\n",
    "            n_step (int, optional): Total number of steps. Defaults to 1.\n",
    "            n_total (int, optional): Total number of feature transformer blocks. Defaults to 4.\n",
    "            n_shared (int, optional): Number of shared feature transformer blocks. Defaults to 2.\n",
    "            relaxation_factor (float, optional): >1 will allow features to be used more than once. Defaults to 1.5.\n",
    "            bn_epsilon (float, optional): Batch normalization, epsilon. Defaults to 1e-5.\n",
    "            bn_momentum (float, optional): Batch normalization, momentum. Defaults to 0.7.\n",
    "            bn_virtual_divider (int, optional): Batch normalization. Full batch will be divided by this.\n",
    "        \"\"\"\n",
    "        super(TabNet, self).__init__()\n",
    "        self.output_dim, self.num_features = output_dim, num_features\n",
    "        self.n_step, self.relaxation_factor = n_step, relaxation_factor\n",
    "        self.feature_columns = feature_columns\n",
    "\n",
    "        if feature_columns is not None:\n",
    "            self.input_features = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "        # ? Switch to Ghost Batch Normalization\n",
    "        self.bn = tf.keras.layers.BatchNormalization(\n",
    "            momentum=bn_momentum, epsilon=bn_epsilon\n",
    "        )\n",
    "\n",
    "        kargs = {\n",
    "            \"feature_dim\": feature_dim + output_dim,\n",
    "            \"n_total\": n_total,\n",
    "            \"n_shared\": n_shared,\n",
    "            \"bn_momentum\": bn_momentum,\n",
    "            \"bn_virtual_divider\": bn_virtual_divider,\n",
    "        }\n",
    "\n",
    "        # first feature transformer block is built first to get the shared blocks\n",
    "        self.feature_transforms: List[FeatureTransformer] = [\n",
    "            FeatureTransformer(**kargs)\n",
    "        ]\n",
    "        self.attentive_transforms: List[AttentiveTransformer] = []\n",
    "        for i in range(n_step):\n",
    "            self.feature_transforms.append(\n",
    "                FeatureTransformer(**kargs, fcs=self.feature_transforms[0].shared_fcs)\n",
    "            )\n",
    "            self.attentive_transforms.append(\n",
    "                AttentiveTransformer(num_features, bn_momentum, bn_virtual_divider)\n",
    "            )\n",
    "\n",
    "    def call(\n",
    "        self, features: tf.Tensor, training: bool = None, alpha: float = 0.0\n",
    "    ) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        if self.feature_columns is not None:\n",
    "            features = self.input_features(features)\n",
    "\n",
    "        bs = tf.shape(features)[0]\n",
    "        out_agg = tf.zeros((bs, self.output_dim))\n",
    "        prior_scales = tf.ones((bs, self.num_features))\n",
    "        masks = []\n",
    "\n",
    "        features = self.bn(features, training=training)\n",
    "        masked_features = features\n",
    "\n",
    "        total_entropy = 0.0\n",
    "\n",
    "        for step_i in range(self.n_step + 1):\n",
    "            x = self.feature_transforms[step_i](\n",
    "                masked_features, training=training, alpha=alpha\n",
    "            )\n",
    "\n",
    "            if step_i > 0:\n",
    "                out = tf.keras.activations.relu(x[:, : self.output_dim])\n",
    "                out_agg += out\n",
    "\n",
    "            # no need to build the features mask for the last step\n",
    "            if step_i < self.n_step:\n",
    "                x_for_mask = x[:, self.output_dim :]\n",
    "\n",
    "                mask_values = self.attentive_transforms[step_i](\n",
    "                    x_for_mask, prior_scales, training=training, alpha=alpha\n",
    "                )\n",
    "\n",
    "                # relaxation factor of 1 forces the feature to be only used once.\n",
    "                prior_scales *= self.relaxation_factor - mask_values\n",
    "\n",
    "                masked_features = tf.multiply(mask_values, features)\n",
    "\n",
    "                # entropy is used to penalize the amount of sparsity in feature selection\n",
    "                total_entropy = tf.reduce_mean(\n",
    "                    tf.reduce_sum(\n",
    "                        tf.multiply(mask_values, tf.math.log(mask_values + 1e-15)),\n",
    "                        axis=1,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                masks.append(tf.expand_dims(tf.expand_dims(mask_values, 0), 3))\n",
    "\n",
    "        loss = total_entropy / self.n_step\n",
    "\n",
    "        return out_agg, loss, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de596503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        feature_dim: int,\n",
    "        output_dim: int,\n",
    "        n_classes: int,\n",
    "        feature_columns: List = None,\n",
    "        n_step: int = 1,\n",
    "        n_total: int = 4,\n",
    "        n_shared: int = 2,\n",
    "        relaxation_factor: float = 1.5,\n",
    "        sparsity_coefficient: float = 1e-5,\n",
    "        bn_epsilon: float = 1e-5,\n",
    "        bn_momentum: float = 0.7,\n",
    "        bn_virtual_divider: int = 32,\n",
    "        dp: float = None,\n",
    "        output_activation: str = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(TabNetClassifier, self).__init__()\n",
    "\n",
    "        self.configs = {\n",
    "            \"num_features\": num_features,\n",
    "            \"feature_dim\": feature_dim,\n",
    "            \"output_dim\": output_dim,\n",
    "            \"n_classes\": n_classes,\n",
    "            \"feature_columns\": feature_columns,\n",
    "            \"n_step\": n_step,\n",
    "            \"n_total\": n_total,\n",
    "            \"n_shared\": n_shared,\n",
    "            \"relaxation_factor\": relaxation_factor,\n",
    "            \"sparsity_coefficient\": sparsity_coefficient,\n",
    "            \"bn_epsilon\": bn_epsilon,\n",
    "            \"bn_momentum\": bn_momentum,\n",
    "            \"bn_virtual_divider\": bn_virtual_divider,\n",
    "            \"dp\": dp,\n",
    "            \"output_activation\": output_activation,\n",
    "        }\n",
    "        for k, v in kwargs.items():\n",
    "            self.configs[k] = v\n",
    "\n",
    "        self.sparsity_coefficient = sparsity_coefficient\n",
    "\n",
    "        self.model = TabNet(\n",
    "            feature_columns=feature_columns,\n",
    "            num_features=num_features,\n",
    "            feature_dim=feature_dim,\n",
    "            output_dim=output_dim,\n",
    "            n_step=n_step,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            bn_epsilon=bn_epsilon,\n",
    "            bn_momentum=bn_momentum,\n",
    "            bn_virtual_divider=bn_virtual_divider,\n",
    "        )\n",
    "        self.dp = tf.keras.layers.Dropout(dp) if dp is not None else dp\n",
    "        self.head = tf.keras.layers.Dense(n_classes, activation=output_activation, use_bias=False)\n",
    "\n",
    "    def call(self, x, training: bool = None, alpha: float = 0.0):\n",
    "        out, sparse_loss, _ = self.model(x, training=training, alpha=alpha)\n",
    "        if self.dp is not None:\n",
    "            out = self.dp(out, training=training)\n",
    "        y = self.head(out, training=training)\n",
    "\n",
    "        if training:\n",
    "            self.add_loss(-self.sparsity_coefficient * sparse_loss)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def get_config(self):\n",
    "        return self.configs\n",
    "\n",
    "    def save_to_directory(self, path_to_folder: Text):\n",
    "        self.save_weights(os.path.join(path_to_folder, \"ckpt\"), overwrite=True)\n",
    "        with open(os.path.join(path_to_folder, \"configs.pickle\"), \"wb\") as f:\n",
    "            pickle.dump(self.configs, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_directory(cls, path_to_folder: Text):\n",
    "        with open(os.path.join(path_to_folder, \"configs.pickle\"), \"rb\") as f:\n",
    "            configs = pickle.load(f)\n",
    "        model: tf.keras.Model = cls(**configs)\n",
    "        model.build((None, configs[\"num_features\"]))\n",
    "        load_status = model.load_weights(os.path.join(path_to_folder, \"ckpt\"))\n",
    "        load_status.expect_partial()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20857e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TabNetClassifier(num_features =512,\n",
    "        feature_dim = 32,\n",
    "        output_dim = 32,\n",
    "        n_classes = 5,\n",
    "        feature_columns = ,\n",
    "        n_step: int = 1,\n",
    "        n_total: int = 4,\n",
    "        n_shared: int = 2,\n",
    "        relaxation_factor: float = 1.5,\n",
    "        sparsity_coefficient: float = 1e-5,\n",
    "        bn_epsilon: float = 1e-5,\n",
    "        bn_momentum: float = 0.7,\n",
    "        bn_virtual_divider: int = 32,\n",
    "        dp: float = None,\n",
    "        output_activation: str = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
